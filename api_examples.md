# API Scraper - Exemples d'utilisation

## üöÄ D√©marrage rapide

### 1. Configuration du token

```bash
# G√©n√©rer un token s√©curis√©
python3 api_config.py
```

### 2. D√©marrage de l'API

```bash
# D√©marrer le serveur API
python3 start_api.py
```

L'API sera disponible sur : **http://localhost:8000**

## üîë Authentification

Toutes les requ√™tes n√©cessitent un bearer token dans l'en-t√™te :

```
Authorization: Bearer your-token-here
```

## üìö Endpoints disponibles

### Health Check

```bash
curl http://localhost:8000/
```

### Documentation interactive

- **Swagger UI** : http://localhost:8000/docs
- **ReDoc** : http://localhost:8000/redoc

## üî• Exemples d'utilisation

### 1. Scraping rapide (param√®tres par d√©faut)

**CURL :**

```bash
curl -X POST "http://localhost:8000/scrape/quick" \
  -H "Authorization: Bearer YOUR_TOKEN_HERE" \
  -H "Content-Type: application/json" \
  -d '{
    "skip_scraping": false,
    "skip_embedding": false,
    "workers": 8
  }'
```

**Python requests :**

```python
import requests

url = "http://localhost:8000/scrape/quick"
headers = {
    "Authorization": "Bearer YOUR_TOKEN_HERE",
    "Content-Type": "application/json"
}
data = {
    "skip_scraping": False,
    "skip_embedding": False,
    "workers": 8
}

response = requests.post(url, headers=headers, json=data)
job = response.json()
print(f"Job cr√©√© : {job['job_id']}")
```

**JavaScript/Node.js :**

```javascript
const axios = require("axios");

const response = await axios.post(
  "http://localhost:8000/scrape/quick",
  {
    skip_scraping: false,
    skip_embedding: false,
    workers: 8,
  },
  {
    headers: {
      Authorization: "Bearer YOUR_TOKEN_HERE",
      "Content-Type": "application/json",
    },
  }
);

console.log("Job cr√©√© :", response.data.job_id);
```

### 2. Scraping personnalis√©

**CURL :**

```bash
curl -X POST "http://localhost:8000/scrape" \
  -H "Authorization: Bearer YOUR_TOKEN_HERE" \
  -H "Content-Type: application/json" \
  -d '{
    "sitemaps": [
      "https://monservicepublic.gouv.mc/sitemap-1.xml",
      "https://monservicepublic.gouv.mc/sitemap-2.xml"
    ],
    "output_folder": "custom_output",
    "thematique": "test",
    "workers": 4,
    "skip_scraping": false,
    "skip_embedding": false
  }'
```

### 3. Suivre un job avec statistiques d√©taill√©es

**CURL - Progr√®s simplifi√© :**

```bash
# R√©cup√©rer le progr√®s en temps r√©el (l√©ger)
curl -X GET "http://localhost:8000/jobs/YOUR_JOB_ID/progress" \
  -H "Authorization: Bearer YOUR_TOKEN_HERE"
```

**CURL - Statistiques compl√®tes :**

```bash
# R√©cup√©rer toutes les statistiques d√©taill√©es
curl -X GET "http://localhost:8000/jobs/YOUR_JOB_ID/stats" \
  -H "Authorization: Bearer YOUR_TOKEN_HERE"
```

**Python script complet avec suivi d√©taill√© :**

```python
import requests
import time
import json

TOKEN = "YOUR_TOKEN_HERE"
BASE_URL = "http://localhost:8000"

headers = {"Authorization": f"Bearer {TOKEN}"}

# 1. Lancer un job
response = requests.post(
    f"{BASE_URL}/scrape/quick",
    headers=headers,
    json={"workers": 4}
)

job_id = response.json()["job_id"]
print(f"üöÄ Job lanc√© : {job_id}")

# 2. Suivre le progr√®s avec statistiques d√©taill√©es
while True:
    # R√©cup√©rer le progr√®s (version l√©g√®re)
    response = requests.get(f"{BASE_URL}/jobs/{job_id}/progress", headers=headers)
    progress = response.json()

    status = progress["status"]
    phase = progress["current_phase"]
    percentage = progress["progress_percentage"]
    urls_done = progress["urls_processed"]
    urls_total = progress["urls_total"]
    vectors = progress["vectors_created"]

    print(f"üìä {status.upper()} | {phase} | {percentage}% | URLs: {urls_done}/{urls_total} | Vecteurs: {vectors}")

    if progress["is_completed"]:
        break

    time.sleep(5)  # V√©rifier toutes les 5 secondes

# 3. R√©cup√©rer les statistiques finales compl√®tes
if status == "completed":
    response = requests.get(f"{BASE_URL}/jobs/{job_id}/stats", headers=headers)
    final_stats = response.json()

    print("\nüéâ SCRAPING TERMIN√â !")
    print("="*50)
    print(f"‚è±Ô∏è  Dur√©e totale : {final_stats['duration']}")

    if final_stats.get('summary'):
        summary = final_stats['summary']
        print(f"üìÑ URLs scrap√©es : {summary['urls_scraped']}")
        print(f"‚ùå URLs √©chou√©es : {summary['urls_failed']}")
        print(f"üìÅ Fichiers cr√©√©s : {summary['files_created']}")
        print(f"üè¢ Services annuaire : {summary['annuaire_services']}")
        print(f"üî¢ Vecteurs cr√©√©s : {summary['vectors_created']}")

    stats = final_stats.get('stats', {})
    print(f"üìÇ Dossiers cr√©√©s : {stats.get('directories_created', 0)}")

else:
    print(f"‚ùå Erreur : {progress.get('error', 'Erreur inconnue')}")
```

**JavaScript/Node.js avec suivi temps r√©el :**

```javascript
const axios = require("axios");

const TOKEN = "YOUR_TOKEN_HERE";
const BASE_URL = "http://localhost:8000";

const headers = { Authorization: `Bearer ${TOKEN}` };

async function runScrapingJob() {
  try {
    // 1. Lancer le job
    const jobResponse = await axios.post(
      `${BASE_URL}/scrape/quick`,
      { workers: 4 },
      { headers }
    );

    const jobId = jobResponse.data.job_id;
    console.log(`üöÄ Job lanc√© : ${jobId}`);

    // 2. Suivre le progr√®s
    let isCompleted = false;
    while (!isCompleted) {
      const progressResponse = await axios.get(
        `${BASE_URL}/jobs/${jobId}/progress`,
        { headers }
      );

      const progress = progressResponse.data;

      console.log(
        `üìä ${progress.status.toUpperCase()} | ${progress.current_phase} | ${
          progress.progress_percentage
        }% | URLs: ${progress.urls_processed}/${progress.urls_total}`
      );

      isCompleted = progress.is_completed;

      if (!isCompleted) {
        await new Promise((resolve) => setTimeout(resolve, 5000)); // 5 secondes
      }
    }

    // 3. Statistiques finales
    const statsResponse = await axios.get(`${BASE_URL}/jobs/${jobId}/stats`, {
      headers,
    });

    const finalStats = statsResponse.data;

    if (finalStats.status === "completed") {
      console.log("\nüéâ SCRAPING TERMIN√â !");
      console.log(`‚è±Ô∏è  Dur√©e : ${finalStats.duration}`);

      if (finalStats.summary) {
        const s = finalStats.summary;
        console.log(`üìä R√©sultats :`);
        console.log(`   URLs scrap√©es : ${s.urls_scraped}`);
        console.log(`   Fichiers cr√©√©s : ${s.files_created}`);
        console.log(`   Vecteurs cr√©√©s : ${s.vectors_created}`);
        console.log(`   Services annuaire : ${s.annuaire_services}`);
      }
    } else {
      console.log(`‚ùå Erreur : ${finalStats.error}`);
    }
  } catch (error) {
    console.error("Erreur :", error.response?.data || error.message);
  }
}

runScrapingJob();
```

### 4. Jobs actifs seulement

**CURL :**

```bash
# Lister uniquement les jobs en cours
curl -X GET "http://localhost:8000/jobs/active" \
  -H "Authorization: Bearer YOUR_TOKEN_HERE"
```

### 5. Lister tous les jobs

**CURL :**

```bash
curl -X GET "http://localhost:8000/jobs" \
  -H "Authorization: Bearer YOUR_TOKEN_HERE"
```

### 6. Supprimer un job

**CURL :**

```bash
curl -X DELETE "http://localhost:8000/jobs/YOUR_JOB_ID" \
  -H "Authorization: Bearer YOUR_TOKEN_HERE"
```

## üìä R√©ponses de l'API

### Job cr√©√© avec succ√®s

```json
{
  "job_id": "123e4567-e89b-12d3-a456-426614174000",
  "status": "pending",
  "message": "Job de scraping cr√©√© et d√©marr√©"
}
```

### Progr√®s d'un job (GET /jobs/{id}/progress)

```json
{
  "job_id": "123e4567-e89b-12d3-a456-426614174000",
  "status": "running",
  "progress_text": "Scraping: 150/1500 URLs (10.0%)",
  "progress_percentage": 10.0,
  "current_phase": "scraping",
  "urls_processed": 150,
  "urls_total": 1500,
  "vectors_created": 0,
  "is_completed": false,
  "error": null
}
```

### Statistiques compl√®tes (GET /jobs/{id}/stats)

```json
{
  "job_id": "123e4567-e89b-12d3-a456-426614174000",
  "status": "completed",
  "current_phase": "completed",
  "progress": "‚úÖ Termin√© en 15m 32s",
  "duration": "15m 32s",
  "stats": {
    "urls_total": 1500,
    "urls_processed": 1485,
    "urls_failed": 15,
    "annuaire_services": 71,
    "vectors_created": 3247,
    "directories_created": 12,
    "files_created": 1485,
    "total_duration_seconds": 932.5,
    "total_duration_formatted": "15m 32s",
    "current_phase": "completed"
  },
  "summary": {
    "urls_scraped": 1485,
    "urls_failed": 15,
    "files_created": 1485,
    "annuaire_services": 71,
    "vectors_created": 3247,
    "total_time": "15m 32s"
  },
  "error": null
}
```

### Status complet d'un job (GET /jobs/{id})

```json
{
  "job_id": "123e4567-e89b-12d3-a456-426614174000",
  "status": "running",
  "created_at": "2025-07-31T18:30:00.000Z",
  "started_at": "2025-07-31T18:30:05.000Z",
  "progress": "Phase 2/2: G√©n√©ration des embeddings...",
  "parameters": {
    "sitemaps": ["https://monservicepublic.gouv.mc/sitemap-1.xml", "..."],
    "workers": 8,
    "skip_scraping": false,
    "skip_embedding": false
  },
  "stats": {
    "urls_total": 1500,
    "urls_processed": 1500,
    "urls_failed": 0,
    "vectors_created": 1250,
    "current_phase": "embedding"
  }
}
```

### Jobs actifs (GET /jobs/active)

```json
{
  "active_jobs": [
    {
      "job_id": "123...",
      "status": "running",
      "progress": "Scraping: 500/1500 URLs (33.3%)",
      "current_phase": "scraping",
      "created_at": "2025-07-31T18:30:00.000Z",
      "started_at": "2025-07-31T18:30:05.000Z"
    }
  ],
  "count": 1
}
```

### Job termin√© avec erreur

```json
{
  "job_id": "123e4567-e89b-12d3-a456-426614174000",
  "status": "failed",
  "progress": "‚ùå Erreur apr√®s 5m 23s: OpenAI API rate limit exceeded",
  "error": "OpenAI API rate limit exceeded",
  "error_type": "RateLimitError",
  "stats": {
    "urls_processed": 450,
    "urls_total": 1500,
    "urls_failed": 1,
    "total_duration_formatted": "5m 23s",
    "current_phase": "failed"
  }
}
```

## ‚ö†Ô∏è Codes d'erreur

- **401 Unauthorized** : Token invalide ou manquant
- **404 Not Found** : Job non trouv√©
- **422 Validation Error** : Param√®tres invalides
- **500 Internal Error** : Erreur serveur

## üîí S√©curit√©

1. **Token s√©curis√©** : Utilisez un token long et al√©atoire
2. **HTTPS** : En production, utilisez HTTPS uniquement
3. **Firewall** : Limitez l'acc√®s √† l'API aux IPs autoris√©es
4. **Logs** : Surveillez les logs d'acc√®s

## üê≥ D√©ploiement Docker (optionnel)

```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 8000
CMD ["python3", "start_api.py"]
```

## üì± Webhooks (future)

L'API pourrait √™tre √©tendue pour envoyer des notifications webhook quand un job se termine :

```json
{
  "job_id": "123...",
  "status": "completed",
  "webhook_url": "https://your-app.com/webhook/scraper"
}
```
