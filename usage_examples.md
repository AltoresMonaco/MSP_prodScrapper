# Guide d'utilisation pratique - API Scraper

## ğŸ¯ Vos 3 cas d'usage principaux

### 1ï¸âƒ£ **Scraping + Embedding COMPLET** (tous sitemaps automatiquement)

**CURL :**

```bash
# Lancer le processus complet (scraping + embedding)
curl -X POST "http://localhost:8000/scrape/full" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"workers": 8}'

# RÃ©ponse :
# {
#   "job_id": "abc123-def456-...",
#   "status": "pending",
#   "message": "Job de scraping crÃ©Ã© et dÃ©marrÃ©"
# }
```

**Python :**

```python
import requests

headers = {"Authorization": "Bearer YOUR_TOKEN"}

# Lancer le processus complet
response = requests.post(
    "http://localhost:8000/scrape/full",
    headers=headers,
    json={"workers": 8}
)

job = response.json()
job_id = job["job_id"]
print(f"ğŸš€ Processus complet lancÃ© : {job_id}")
```

---

### 2ï¸âƒ£ **Embedding SEUL** (sur fichiers existants)

**CURL :**

```bash
# Lancer seulement l'embedding (suppose que les fichiers existent dans output/)
curl -X POST "http://localhost:8000/embedding/run" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "output_folder": "output",
    "thematique": "monservicepublic"
  }'
```

**Python :**

```python
# Lancer seulement l'embedding
response = requests.post(
    "http://localhost:8000/embedding/run",
    headers=headers,
    json={
        "output_folder": "output",
        "thematique": "monservicepublic"
    }
)

job = response.json()
job_id = job["job_id"]
print(f"ğŸ”¢ Embedding seul lancÃ© : {job_id}")
```

---

### 3ï¸âƒ£ **VÃ©rifier l'Ã©tat COMPLET** (simple et efficace)

**CURL :**

```bash
# VÃ©rifier l'Ã©tat d'un job de maniÃ¨re simple
curl -X GET "http://localhost:8000/status/simple/YOUR_JOB_ID" \
  -H "Authorization: Bearer YOUR_TOKEN"
```

**Python - Suivi complet automatisÃ© :**

```python
import time

def monitor_job_simple(job_id):
    """Suit un job jusqu'Ã  completion avec Ã©tat simple"""

    print(f"ğŸ“Š Suivi du job {job_id}...")

    while True:
        response = requests.get(
            f"http://localhost:8000/status/simple/{job_id}",
            headers=headers
        )

        status_data = response.json()

        # Infos de base
        status = status_data["status"]
        phase = status_data["current_phase"]
        progress = status_data["progress_text"]

        print(f"ğŸ“ˆ {status.upper()} | {phase} | {progress}")

        # Si terminÃ©
        if status_data["is_completed"]:
            print("âœ… JOB TERMINÃ‰ !")
            print(f"   ğŸ“„ URLs scrapÃ©es : {status_data.get('urls_scraped', 0)}")
            print(f"   ğŸ”¢ Vecteurs crÃ©Ã©s : {status_data.get('vectors_created', 0)}")
            print(f"   ğŸ“ Fichiers crÃ©Ã©s : {status_data.get('files_created', 0)}")
            print(f"   ğŸ¢ Services annuaire : {status_data.get('annuaire_services', 0)}")
            print(f"   â±ï¸  Temps total : {status_data.get('total_time', 'N/A')}")
            break

        # Si erreur
        elif status_data["is_failed"]:
            print("âŒ JOB Ã‰CHOUÃ‰ !")
            print(f"   Erreur : {status_data.get('error', 'Inconnue')}")
            break

        # Si en cours
        elif status_data["is_running"]:
            urls_done = status_data.get("urls_processed", 0)
            urls_total = status_data.get("urls_total", 0)
            vectors = status_data.get("vectors_created", 0)

            if urls_total > 0:
                pct = (urls_done / urls_total) * 100
                print(f"   Progress: {pct:.1f}% | URLs: {urls_done}/{urls_total} | Vecteurs: {vectors}")

        time.sleep(5)  # VÃ©rifier toutes les 5 secondes

# Utilisation
job_id = "votre-job-id-ici"
monitor_job_simple(job_id)
```

---

## ğŸ”¥ **Script complet - Workflow type**

```python
import requests
import time

TOKEN = "YOUR_TOKEN_HERE"
BASE_URL = "http://localhost:8000"
headers = {"Authorization": f"Bearer {TOKEN}"}

def complete_workflow():
    """Workflow complet : lancer + surveiller"""

    print("ğŸš€ DÃ‰MARRAGE WORKFLOW COMPLET")
    print("="*50)

    # 1. Lancer le processus complet
    print("\n1ï¸âƒ£ Lancement scraping + embedding complet...")

    response = requests.post(
        f"{BASE_URL}/scrape/full",
        headers=headers,
        json={"workers": 8}
    )

    if response.status_code != 200:
        print(f"âŒ Erreur : {response.text}")
        return

    job = response.json()
    job_id = job["job_id"]

    print(f"âœ… Job crÃ©Ã© : {job_id}")
    print(f"ğŸ“Š Status initial : {job['status']}")

    # 2. Surveiller jusqu'Ã  completion
    print(f"\n2ï¸âƒ£ Surveillance en cours...")
    print("-" * 60)

    while True:
        # VÃ©rifier l'Ã©tat
        response = requests.get(
            f"{BASE_URL}/status/simple/{job_id}",
            headers=headers
        )

        status = response.json()

        print(f"ğŸ“ˆ {status['status'].upper():10} | {status['current_phase']:12} | {status['progress_text']}")

        if status["is_completed"]:
            print("\nğŸ‰ PROCESSUS TERMINÃ‰ AVEC SUCCÃˆS !")
            print("=" * 50)
            print(f"ğŸ“Š RÃ©sultats finaux :")
            print(f"   ğŸ“„ URLs scrapÃ©es      : {status.get('urls_scraped', 0):,}")
            print(f"   ğŸ“ Fichiers crÃ©Ã©s     : {status.get('files_created', 0):,}")
            print(f"   ğŸ¢ Services annuaire  : {status.get('annuaire_services', 0)}")
            print(f"   ğŸ”¢ Vecteurs crÃ©Ã©s     : {status.get('vectors_created', 0):,}")
            print(f"   â±ï¸  Temps total       : {status.get('total_time', 'N/A')}")
            break

        elif status["is_failed"]:
            print(f"\nâŒ PROCESSUS Ã‰CHOUÃ‰ !")
            print(f"   Erreur : {status.get('error', 'Inconnue')}")
            break

        time.sleep(10)  # Attendre 10 secondes

    print("\nâœ… Workflow terminÃ© !")

if __name__ == "__main__":
    complete_workflow()
```

---

## ğŸ“‹ **RÃ©ponses des nouveaux endpoints**

### `/scrape/full` - Job crÃ©Ã©

```json
{
  "job_id": "abc123-def456-789",
  "status": "pending",
  "message": "Job de scraping crÃ©Ã© et dÃ©marrÃ©"
}
```

### `/embedding/run` - Job embedding

```json
{
  "job_id": "xyz789-abc123-456",
  "status": "pending",
  "message": "Job de scraping crÃ©Ã© et dÃ©marrÃ©"
}
```

### `/status/simple/{id}` - Ã‰tat en cours

```json
{
  "job_id": "abc123-def456-789",
  "status": "running",
  "is_running": true,
  "is_completed": false,
  "is_failed": false,
  "progress_text": "Scraping: 450/1500 URLs (30.0%)",
  "current_phase": "scraping",
  "urls_processed": 450,
  "urls_total": 1500,
  "vectors_created": 0
}
```

### `/status/simple/{id}` - Ã‰tat terminÃ©

```json
{
  "job_id": "abc123-def456-789",
  "status": "completed",
  "is_running": false,
  "is_completed": true,
  "is_failed": false,
  "progress_text": "âœ… TerminÃ© en 15m 32s",
  "current_phase": "completed",
  "urls_scraped": 1485,
  "vectors_created": 3247,
  "total_time": "15m 32s",
  "files_created": 1485,
  "annuaire_services": 71
}
```

---

## ğŸ¯ **RÃ©sumÃ© pour vous**

```bash
# 1. Processus COMPLET (tous sitemaps automatiquement)
curl -X POST "http://localhost:8000/scrape/full" -H "Authorization: Bearer TOKEN" -d '{"workers":8}'

# 2. Embedding SEUL (fichiers existants)
curl -X POST "http://localhost:8000/embedding/run" -H "Authorization: Bearer TOKEN" -d '{}'

# 3. Ã‰tat SIMPLE (vÃ©rification facile)
curl -X GET "http://localhost:8000/status/simple/JOB_ID" -H "Authorization: Bearer TOKEN"
```

**C'est exactement ce que vous vouliez !** ğŸ‰
